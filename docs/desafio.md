# ğŸ§  IngestÃ£o e Busca SemÃ¢ntica com LangChain e Postgres

## ğŸ¯ Objetivo

VocÃª deve entregar um software capaz de:

- **IngestÃ£o:** Ler um arquivo PDF e salvar suas informaÃ§Ãµes em um banco de dados PostgreSQL com extensÃ£o **pgVector**.
- **Busca:** Permitir que o usuÃ¡rio faÃ§a perguntas via linha de comando (CLI) e receba respostas **baseadas apenas no conteÃºdo do PDF**.

---

## ğŸ’¬ Exemplo no CLI

```
FaÃ§a sua pergunta:

PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milhÃµes de reais.
```

---

### Perguntas fora do contexto

```
PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

---

## âš™ï¸ Tecnologias obrigatÃ³rias

- **Linguagem:** Python
- **Framework:** LangChain
- **Banco de dados:** PostgreSQL + pgVector
- **ExecuÃ§Ã£o do banco:** Docker & Docker Compose (docker-compose fornecido no repositÃ³rio de exemplo)

---

## ğŸ“¦ Pacotes recomendados

```python
# Split
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Embeddings (OpenAI)
from langchain_openai import OpenAIEmbeddings

# Embeddings (Gemini)
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# PDF
from langchain_community.document_loaders import PyPDFLoader

# IngestÃ£o
from langchain_postgres import PGVector

# Busca
similarity_search_with_score(query, k=10)
```

---

## ğŸ”‘ OpenAI

- Crie uma **API Key** da OpenAI.
- **Modelo de embeddings:** `text-embedding-3-small`
- **Modelo de LLM para responder:** `gpt-5-nano`

---

## ğŸ”‘ Gemini

- Crie uma **API Key** da Google.
- **Modelo de embeddings:** `models/embedding-001`
- **Modelo de LLM para responder:** `gemini-2.5-flash-lite`

---

## ğŸ“‹ Requisitos

### 1) IngestÃ£o do PDF

- O PDF deve ser dividido em **chunks de 1000 caracteres** com **overlap de 150**.
- Cada chunk deve ser convertido em **embedding**.
- Os vetores devem ser armazenados no banco de dados PostgreSQL com **pgVector**.

### 2) Consulta via CLI

Crie um script Python para simular um chat no terminal.

Passos ao receber uma pergunta:
1. Vetorizar a pergunta.
2. Buscar os **10 resultados mais relevantes (k=10)** no banco vetorial.
3. Montar o **prompt** e chamar a **LLM**.
4. Retornar a resposta ao usuÃ¡rio.

---

## ğŸ§© Prompt a ser utilizado

```
CONTEXTO:
{resultados concatenados do banco de dados}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informaÃ§Ã£o nÃ£o estiver explicitamente no CONTEXTO, responda:
  "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opiniÃµes ou interpretaÃ§Ãµes alÃ©m do que estÃ¡ escrito.

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
Pergunta: "Qual Ã© a capital da FranÃ§a?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "Quantos clientes temos em 2024?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "VocÃª acha isso bom ou ruim?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

PERGUNTA DO USUÃRIO:
{pergunta do usuÃ¡rio}

RESPONDA A "PERGUNTA DO USUÃRIO"
```

---

## ğŸ§± Estrutura obrigatÃ³ria do projeto

FaÃ§a um fork do repositÃ³rio base e utilize a seguinte estrutura:

```
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt      # DependÃªncias
â”œâ”€â”€ .env.example          # Template da variÃ¡vel OPENAI_API_KEY
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py         # Script de ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py         # Script de busca
â”‚   â”œâ”€â”€ chat.py           # CLI para interaÃ§Ã£o com usuÃ¡rio
â”œâ”€â”€ document.pdf          # PDF para ingestÃ£o
â””â”€â”€ README.md             # InstruÃ§Ãµes de execuÃ§Ã£o
```

---

## ğŸ”— RepositÃ³rios Ãºteis

- /Users/edney/projects/full-cycle/mba/mba-ia-niv-introducao-langchain

---

## ğŸ§° VirtualEnv para Python

Crie e ative um ambiente virtual antes de instalar as dependÃªncias:

```bash
python3 -m venv venv
source venv/bin/activate
```

---

## ğŸš€ Ordem de execuÃ§Ã£o

1. Subir o banco de dados:

```bash
docker compose up -d
```

2. Executar ingestÃ£o do PDF:

```bash
python src/ingest.py
```

3. Rodar o chat:

```bash
python src/chat.py
```

---

## ğŸ“¦ EntregÃ¡vel

RepositÃ³rio pÃºblico no **GitHub** contendo:

- Todo o **cÃ³digo-fonte**
- **README** com instruÃ§Ãµes claras de execuÃ§Ã£o do projeto

